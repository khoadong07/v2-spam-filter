import json
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer

LABEL_MAPPING = {"LABEL_0": "spam", "LABEL_1": "non-spam"}

CATEGORY_MODEL_MAP = {
    "finance": "Khoa/kompa-spam-filter-finance-update-0525",
    "real_estate": "Khoa/kompa-spam-filter-real-estate-update-0525",
    "ewallet": "Khoa/kompa-spam-filter-e-wallet-update-0625",
    "healthcare_insurance": "Khoa/kompa-spam-filter-healthcare-insurance-update-0525",
    "ecommerce": "Khoa/kompa-spam-filter-e-commerce-update-0625",
}

def truncate_text(text, tokenizer, max_tokens=254):
    tokens = tokenizer.tokenize(text)
    if len(tokens) <= max_tokens:
        return text
    truncated_ids = tokenizer.convert_tokens_to_ids(tokens[:max_tokens])
    return tokenizer.decode(truncated_ids, skip_special_tokens=True)

# Language classifier
lang_classifier = pipeline(
    "text-classification",
    model="papluca/xlm-roberta-base-language-detection",
    device=-1
)

# Cache models
_model_cache = {}

def load_model(category):
    if category in _model_cache:
        return _model_cache[category]
    if category not in CATEGORY_MODEL_MAP:
        raise ValueError(f"âŒ KhÃ´ng cÃ³ mÃ´ hÃ¬nh cho category: {category}")
    
    model_path = CATEGORY_MODEL_MAP[category]
    print(f"ðŸ”§ Loading model for category '{category}' from {model_path}")
    
    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
    classifier = pipeline(
        "text-classification",
        model=model,
        tokenizer=tokenizer,
        device=-1,
        return_all_scores=True
    )
    _model_cache[category] = (classifier, tokenizer)
    return classifier, tokenizer

# HÃ m gá»i inference Ä‘Æ¡n láº»
def run_inference(text, category):
    if not isinstance(text, str) or not text.strip():
        raise ValueError("âš ï¸ Text Ä‘áº§u vÃ o khÃ´ng há»£p lá»‡")

    classifier, tokenizer = load_model(category)
    text = truncate_text(text, tokenizer)

    spam_result = classifier(text)[0]
    spam_result = [r for r in spam_result if r["label"] in LABEL_MAPPING]
    if not spam_result:
        raise ValueError("âš ï¸ Káº¿t quáº£ khÃ´ng há»£p lá»‡ tá»« mÃ´ hÃ¬nh.")

    top_spam = max(spam_result, key=lambda x: x['score'])
    spam_label = top_spam['label']
    spam_score = top_spam['score']

    lang_result = lang_classifier(text, top_k=1)[0]
    lang_label = lang_result['label']
    lang_score = lang_result['score']

    return {
        "spam_label": LABEL_MAPPING.get(spam_label, spam_label),
        "spam_score": spam_score,
        "language": lang_label,
        "language_score": lang_score
    }

# ===== Test demo =====
if __name__ == "__main__":
    test_text = "MÃ n hÃ¬nh Gaming Galax Vivance 32Q (VI-32Q) 32 inch 2K QHD IPS 165Hz 1msðŸ‘‰MÃ n hÃ¬nh 2k Ä‘áº¹p nÃ©t Ä‘áº¿n tá»«ng giÃ¢y luÃ´n >< ðŸŽ ð“ðšÌ£Ì†ð§ð  ð§ð ðšð² ðœð¡ð®ð¨Ì£Ì‚ð­ ð†ð€ðŒðˆðð† ð­ð«ð¢Ì£ ð ð¢ðšÌ ðŸð­ð« Ä‘ð¨Ì‚Ì€ð§ð  ðŸŽ================== âš¡ Há»• Trá»£ Tráº£ gÃ³p HD Saison & Home Creditâš¡ Quáº¹t Tháº» TÃ­n Dá»¥ng MposðŸš› Ship cod toÃ n quá»‘c: Nháº­n hÃ ng kiá»ƒm tra hÃ ng vÃ  thanh toÃ¡n tiá»n cho nhÃ¢n viÃªn giao hÃ ngðŸ¡ Há»¯u TÃ i Computer PC Hi-End - Gaming GearðŸ“Œ Sá»‘ 33/10 Ä‘Æ°á»ng Pháº¡m ThÃ¡i BÆ°á»ng,. P4. ThaÌ€nh PhÃ´Ì ViÌƒnh Long. (Ä‘á»‘i diá»‡n cá»•ng trÆ°Æ¡ng tiá»ƒu há»c Tráº§n Äáº¡i NghÄ©a)ðŸ“Œ CN2 sá»‘ 44B Äinh TiÃªn HoÃ ng P8 (cÃ¡ch báº¿n xe má»›i 300M hÆ°á»›ng Ä‘i Cáº§n ThÆ¡) â˜Ž 0939.182727 - 0939.792727  (zalo Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n nhanh nháº¥t áº¡)ðŸŒ https://maytinhvinhlong.vn/ "
    category = "finance"

    result = run_inference(test_text, category)
    print("âœ… Inference result:")
    print(json.dumps(result, indent=2, ensure_ascii=False))
